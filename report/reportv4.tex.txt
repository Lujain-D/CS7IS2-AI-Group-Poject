\documentclass{svproc}
\usepackage{url}
\usepackage{siunitx}
\usepackage[super]{nth}
\def\UrlFont{\rmfamily}

\begin{document}
\mainmatter
\title{A Survey of Sudoku Solving Algorithms}
\subtitle{CS7IS2 Project (2019/2020)}
\author{Arun Bhargav Ammanamanchi, Lujain Dweikat, Donal Egan, Bharat Vyas}

\institute{
\email{ammanama@tcd.ie}, \email{dweikatl@tcd.ie}, \email{doegan@tcd.ie}, \email{vyasb@tcd.ie}}

\maketitle              % typeset the title of the contribution
\begin{abstract}
The popular Sudoku puzzle is usually solved by humans using a range of inference strategies. While it can take us minutes or even hours to solve a Sudoku, computers can solve them in a fraction of the time using both inference and search methods. In this paper, we carry out a survey of different Sudoku solving algorithms. We test the performance of several algorithms on Sudoku puzzles with three different levels of difficulty - easy, medium, and hard. We analyse the algorithms' performance with respect to three criteria, namely the number of nodes expanded in the search tree, the time taken to find the solution, and the memory usage while searching for the solution. Of the algorithms that we test, we find that the best algorithms for solving a Sudoku puzzle are backtracking search with the minimum-remaining-values heuristic and Algorithm X.
\keywords{Sudoku, constraint-satisfaction, backtracking search, minimum-remaining-value, AC-3, breadth first search, Algorithm X}
\end{abstract}
%

\section{Introduction}
Modern Sudoku, whose name derives from the Japanese translation of the phrase \emph{the digits must be single}, first started gaining popularity in Japan in the 1980s. Since then, the puzzle has become extremely popular with humans all over the world who use different, and often quite complicated, inference strategies to solve them. 

A Sudoku puzzle contains 81 squares (called cells) laid out in a $9\times 9$ grid. This grid is subdivided into nine $3\times3$ sub-grids or boxes. Initially, some cells are filled with digits from 1 to 9. The aim of the game is to fill in the rest of the cells with digits from 1 to 9 so that no digit appears twice in any row, column or $3\times3$ sub-grid. A properly formulated Sudoku puzzle should have a exactly one solution. 

In this paper we evaluate the performance of different algorithms when solving Sudoku puzzles. The chosen algorithms use both inference and search methods to solve the Sudoku. We evaluate the algorithms according to the number of nodes expanded in the search tree, the time taken to solve a puzzle, and the memory usage while solving a puzzle. 

The rest of the paper is structured as follows: In section 2, we provide a review of the related research in the area. In section 3, we formalise our research question and provide detailed descriptions of the algorithms that will be evaluated. In section 4, we provide the methodology for our evaluation, the results of our evaluation, and a discussion of these results. Finally, in Section 5, we provide a brief summary of our work and conclude the paper. 

\section{Related Work}
Sudoku are a classic example of a constraint satisfaction problem (csp). Approaches to solving them can include inference techniques, search methods or a combination of both.

In \cite{mack}, the concept of \emph{arc-consistency} is introduced. Two variables, $X$ and $Y$, in a csp form an arc if they share a binary constraint. The variable $X$ is said to be arc-consistent with respect to $Y$ if for every value $x$ in the domain of $X$ there exists a value in the domain of $Y$ that satisfies the constraint on the arc. A csp is arc-consistent if every variable in the csp is arc-consistent with every other variable. The AC-3 algorithm, also introduced in \cite{mack}, can be used to achieve arc-consistency in a csp. We examine the ability of the AC-3 to solve a Sudoku by itself using only inference. We also use AC-3 as a pre-processing step to reduce the search space before applying  search algorithms. 





The third algorithm we chose is called Algorithm X. Algorithm X is an algorithm described by Donald Knuth in ~\cite{knuth2000dancing} that uses dancing links ~\cite{knuth2000dancing}, a form of two-dimensional circular doubly linked lists, in order to solve an exact cover problem. 


An exact cover problem is an NP-complete problem as described by ~\cite{book_1975}. 
Any NP-complete problem can be translated into an exact cover problem, and since a Sudoku puzzle is a form of an NP-complete problem, it can be translated into an exact cover problem solvable using Algorithm X in a straight-forward way. 


\section{Problem Definition and Algorithm}
A Sudoku puzzle is an example of a constraint satisfaction problem (CSP). Each of the 81 cells in the Sudoku grid is called a \emph{variable}. The \emph{domain} of a variable is the set of values that it can take. Initially, empty cells have domain \{1, 2, 3, 4, 5, 6, 7, 8, 9\}, while pre-filled cells have a domain consisting of the single value entered in the cell. All constraints in a Sudoku puzzle can be expressed as binary constraints. For example, the requirement that all cells the first row contain different values can be expressed as
\[A1\neq A2, A1 \neq A3, \ldots, A2 \neq A3, A2 \neq A4,\ldots,  A7\neq A8, A7 \neq A9, A8 \neq A9.\] A cell's \emph{neighbours} is the set of cells with which it shares a constraint (it does not include the cell itself). Each cell has 20 neighbours. An \emph{assignment} is a partial or complete solution to a Sudoku.

For our research, We analyse the performance of several algorithms and heuristics for solving Sudoku. The following subsections describe the algorithms and heuristics we explore.
\subsection{AC-3}
The AC-3 algorithm introduced by \cite{mack} is an inference algorithm for ensuring arc-consistency in a constraint satisfaction problem (csp). The following pseudocode illustrates the algorithm. The function REVISE() which the algorithm uses it described first:

\vspace{12pt}
\hspace{12pt}\textbf{function} REVISE(\emph{csp}, $X_i$, $X_j$):

\hspace{24pt}\emph{revised} $\longleftarrow$ \emph{false}

\hspace{24pt}\textbf{for each} $x$ in the domain of $X_i$:

\hspace{36pt}\textbf{if} there is no $y$ in the domain of $X_j$ such that $x \neq y$:

\hspace{48pt}remove $x$ from the domain of $X_i$

\hspace{48pt}\emph{revised} $\longleftarrow$ \emph{true}

\hspace{24pt}\textbf{return} \emph{revised}



\vspace{12pt}
\hspace{12pt}\textbf{function} AC-3(\emph{csp}):

\hspace{24pt}\emph{queue} $\longleftarrow$ set of all arcs in \emph{csp}

\hspace{24pt}\textbf{while } \emph{queue } \mbox{is not empty:}

\hspace{36pt}$(X_{i}, X_{j}) \longleftarrow$ next arc in queue

\hspace{36pt}\textbf{if} REVISE($X_i, X_j)$:

\hspace{48pt} \textbf{if} domain of $X_i$ is empty:

\hspace{60pt} \textbf{return} \emph{false}

\hspace{48pt}\textbf{else:}

\hspace{60pt} add arc $(X_k, X_i)$ to \emph{queue} for each neighbour $X_k$ of $X_i$

\hspace{24pt} \textbf{return} \emph{true}


\vspace{12pt}An arc is a pair of variables that share a constraint, i.e. two Sudoku cells that cannot have the same value. For example, the pair of cells $(A1, C3)$ is an arc. Initially, the \emph{queue} consists of all arcs. The first arc, for example $(A1, C3)$, is removed from the queue. For each value $x$ in the domain of $A1$, the algorithm checks if there exists some value in the domain of $C3$ which can satisfy the constraint, i.e. a value different to $x$. If no such value is found, $x$ is removed from the domain of $A1$. If, after all values have been checked, the domain of $A1$ is unchanged the algorithm moves to the next arc. Otherwise, all arcs $(Y, A1)$, where $Y$ is a neighbour of $A1$, are added to the queue. The algorithm runs until the earlier of the \emph{queue} becomes empty or the domain of a variable becomes empty


If the domain of a variable becomes empty, the AC-3 algorithm returns \emph{false} and the constraint satisfaction problem does not have a solution. If the AC-3 algorithm reduces the size of every cell's domain to one, then the Sudoku puzzle has been solved. Otherwise, we can pass the Sudoku puzzle with reduced domain sizes to the backtracking search algorihtm (described below). 

We test the performance of AC-3 as a standalone algorithm for solving Sudoku and also as a preprocessing step to reduce domain sizes before using backtracking search (described below).

\subsection{Backtracking search / Depth first search (DFS)}
\subsubsection{Depth first search}Depth first traversal or Depth first Search is a recursive algorithm for searching all the vertices of a graph or tree data structure.A standard DFS implementation puts each vertex of the graph into one of two categories, Visited and Not Visited.The algorithm works as follows:

1. Start by putting any one of the graph's vertices on top of a stack.

2. Take the top item of the stack and add it to the visited list.

3. Create a list of that vertex's adjacent nodes and add the ones which aren't in the visited list to the top of stack.

4.Keep repeating steps 2 and 3 until the stack is empty.

\vspace{12pt}
\hspace{12pt}\textbf{function} DFS (G, v):

\hspace{24pt}let S be a stack

\hspace{24pt} S.push ( v )

\hspace{24pt}\textbf{while} S i s not empty

\hspace{36pt}v $\longleftarrow$ S . pop ( )

\hspace{36pt}\textbf{if} v is not labeled as discovered:

\hspace{48pt}label v as discovered

\hspace{48pt}\textbf{for} all edges from v to win G. adjacentEdges ( v ) do

\hspace{66pt}S . push (w)

\subsubsection{BackTracking} The Backtracking is a modified DFS of a tree consisting of all of the possible sequences that can be constructed from the problem set. In Backtracking we perform a DFS traversal of this tree until we reach a node that is non-viable or non-promising, at which point we prune the subtree rooted at this node, and continue the DFS traversal of the tree.

\vspace{12pt}
\hspace{12pt}\textbf{function} DFS(c):

\hspace{24pt}\textbf{if} reject(P, c ) then return

\hspace{24pt}\textbf{if} accept (P, c ) then output (P, c )

\hspace{24pt}S  $\longleftarrow$ first (P, c )

\hspace{24pt}\textbf{while} S \textbf{NOT NULL}  do

\hspace{48pt}DFS( S )

\hspace{48pt}S $\longleftarrow$ first (P, S )

\vspace{12pt}
The paper that I have choosen is {Breadth-First Search, Depth-First Search and Backtracking Depth-First Search Algorithms applied on a Sudoku Puzzle Solver by}\cite{bit:junior}, In which they have solved the Sudoku Puzzle using DFS and backtracking.
\subsubsection{Minimum-remaining-values}In the basic backtracking search algorithm described above, the next variable chosen to be assigned a value is just the next variable in the list of unassigned variables. However, there are other ways to choose this next variable. We test whether the \emph{minimum-remaining-values} (MRV) heuristic, introduced by \cite{bit:rein}, improves the performance of backtracking search for solving Sudoku. The idea is to always choose the next variable to be the one that is most likely to cause the current assignment to fail as a solution soon. For example, if the unassigned cell $A1$'s domain is empty while all other unassigned cells' domains are non-empty, the $MRV$ heuristic will choose $A1$ as the next cell to be assigned a value. The failure of the current assignment as a solution to the Sudoku will be immediately detected and time will not be spent expanding the current assignment further before the failure is detected.
 

\subsection{Breadth first search}
Breadth-First Search (BFS) is another graph search algorithm introduced by \cite{moore}, which explores the graph layer by layer. It starts with the root node just like depth-first search (DFS). But unlike DFS, it explores all the neighbour nodes at present layer prior moving to nodes in the next layer. The function BFS() in the following pseudocode illustrates the algorithm. 

\vspace{12pt}
\hspace{12pt}\textbf{function} BFS(\emph{problem}):

\hspace{24pt}\emph{node} $\longleftarrow$ INSERT(MAKE-NODE(INITIAL-STATE[\emph{problem}])

\hspace{24pt}\textbf{if }\mbox{GOAL-TEST(STATE[\emph{node}]):}

\hspace{36pt} \textbf{return }\emph{node}

\hspace{24pt}\emph{frontier} $\longleftarrow$ a FIFO queue with \emph{node} as only element

\hspace{24pt}\emph{explored} $\longleftarrow$ an empty set

\hspace{24pt}\textbf{while } \emph{frontier} \mbox{is not empty:}

\hspace{36pt}\emph{node} $\longleftarrow$ POP (\emph{frontier})

\hspace{36pt}\emph{explored} $\longleftarrow$ PUSH (STATE[\emph{node}])

\hspace{36pt}\textbf{for each} \emph{action} \textbf{in} \mbox ACTIONS({\emph{problem}, STATE[\emph{node}]):} 

\hspace{48pt} \emph{child} $\longleftarrow$ CHILD-NODE(\emph{problem, node, action})

\hspace{48pt} \textbf{if} STATE[\emph{child}] is not in \emph{explored} or \emph{frontier}:

\hspace{60pt} \textbf{if} GOAL-TEST(STATE[\emph{child}]):

\hspace{72pt} \textbf{return} \emph{child}

\hspace{60pt} \emph{frontier} $\longleftarrow$ INSERTALL(\emph{child}, \emph{frontier})

\vspace{12pt}The search algorithms work by considering various possible action sequences, with solution being one of them. This algorithm starts with initial state as being the root node of graph. A\emph{node} correspond to a state in the state space of problem graph, with \emph{actions} as its branches. 


The first step is to check if root node is the solution state i.e, if problem is already solved by using function GOAL-TEST(). Then, a FIFO(First In First Out) queue \emph{frontier} is defined with just root node (initial state) in it. Now, root node is expanded into child nodes, by finding first empty cell and considering only domain values of that cell. For example, if A2 is the first empty ce
ll and have 3 different domain values, then there are three possible action sequences in this layer. This means three child nodes are generated from root node after expansion. After each expansion, \emph{frontier} is updated and every node or state is checked for solution.

Similarly, each node is further expanded into child nodes. The \emph{explored} which is initially defined as an empty set, keep the record of all the explored nodes for solution. The newly generated nodes which matches the nodes in \emph{explored} set are discarded and not added to \emph{frontier}. So, the nodes in the \emph{explored} set are not visited again for solution. When the function GOAL-TEST() returns true, the child node or solution node is returned.


\subsection{Algorithm X}
Algorithm X is used to solve exact cover problems on Dancing Links. A Sudoku puzzle is first turned into an exact cover problem, then it's solved using Algorithm X. 
\subsubsection{Dancing Links} data structure was described by Donald Knuth in ~\cite{knuth2000dancing} as a two-dimensional, circular, doubly linked list. Each node in the list has four pointers pointing up, down, left and right. Each node is directly part of a column, and implicitly part of a row (there is no need for a “row pointer”). Columns are also nodes. The column nodes are connected to a header node, which is the root of the data structure. Moving into a single direction through a list from a node should eventually return the pointer to the start node (circularity).  


In order for us to build a set of dancing links out of a Sudoku puzzle, we first need to determine number of columns we’ll need. The number of columns is equal to the total number of constraints in the puzzle. In our chosen Sudoku size, each puzzle has $9 \times 9$ = 81 cells. Each cell is under four constraints
Each cell should contain one digit only -  $81 \times 1$ constraints
Each row should contain nine distinct digits - $9 \times 9$ constraints
Each column should contain nine distinct digits - $9 \times 9$ constraints
Each $3 \times 3$ box should contain nine distinct digits - $9 \times 9$ constraints
This leaves us with 81 + 81 + 81 + 81 = 324 constraints that will be transformed into 324 columns. 
We implemented our code to add the constraints in the order they’re presented above. 
The maximum number of rows for a completely empty Sudoku game is $9 \times 9$ $ \times 9$  = 729 rows for nine possible answers per cell. However, due to the fact that we’ll have some cells filled in, we’ll end up with fewer rows. 

Starting from a header node, columns will be added in order one by one until the \nth{324} column is added. This is done for all puzzles similarly. Then, the specific puzzle grid is traversed one cell at a time, adding one row with four nodes into the links per pre-filled cell, carrying the filled digit, and nine per each empty cell on the grid, with the nine potential cell values that could be assigned. 

After each cell in the grid has been visited, populating the dancing links is over.


\vspace{12pt}

\hspace{12pt}\textbf{function} GENERATE DANCING LINKS (\emph{puzzle}):

\hspace{24pt}\emph{NumberOfColumns} $\longleftarrow$ \emph{ $nRows \times nColumns$ $\times nConstarints$}

\hspace{24pt}\emph{create header cell}

\hspace{24pt}\textbf{for} $column$ in range $NumberOfColumns$:

\hspace{36pt}\emph{create column}

\hspace{36pt}\emph{add column to header}

\hspace{24pt}\textbf{for each} $cell$ in $puzzle$:

\hspace{36pt}\textbf{if} cell is empty:

\hspace{48pt}\emph{create rows for all values between 0 to 8}

\hspace{48pt}\emph{link created rows to corresponding columns}

\hspace{36pt}\textbf{else}:

\hspace{48pt}\emph{create rows for the value of the cell - 1}

\hspace{48pt}\emph{link created rows to corresponding columns}


\subsubsection{Algorithm X} starts by choosing a column from our dancing links, in our case, we chose the column with the least number of cells first. We remove or cover that column first.  For each row in that column that has a node, we cover the containing column of that cell. When covering a column, it is removed from the list, but the removed column still retains pointers to its original location in the links. Therefore, if columns are covered then uncovered in the opposite order, we would have effectively and efficiently implemented backtracking. 
If the dancing links are empty by the end of this process, a solution has been reached. However, if covering a column is no longer possible, we start backtracking by uncovering the covered columns in reverse order. 


\vspace{12pt}

\hspace{12pt}\textbf{function} SEARCH ALGORITHM X (\emph{Links}):

\hspace{24pt}\textbf{while} \emph{Links} are not empty:

\hspace{36pt}\emph{choose the \textbf{column} with the least number of elements in it}

\hspace{36pt}\emph{\textbf{cover} the chosen column}

\hspace{36pt}\textbf{for each} row in the covered column:

\hspace{48pt} add row to solution

\hspace{48pt} \textbf{cover} the column of each cell in the row

\hspace{36pt}SEARCH ALGORITHM X (\emph{Links})

\hspace{48pt} remove row from solution

\hspace{48pt} \textbf{uncover} the column of each cell in the row

\hspace{36pt}\emph{\textbf{uncover} the chosen column}








\section{Experimental Results}
\subsection{Methodology}
We test each algorithm on three data sets, each containing fifty Sudoku puzzles. Each of the three datasets has a different difficulty level, namely \emph{easy}, \emph{medium}, and \emph{hard}.\footnote{We constructed the easy and medium datasets ourselves. The difficult data set was obtained from \cite{norvig}.} 

The performance of each algorithm is evaluated according to the following three criteria:
\begin{enumerate}
    \item Number of nodes expanded in the search tree: a node is defined as a value being assigned to a variable in the search tree, i.e. a number being placed in a cell.
    \item Time taken to solve a Sudoku.
    \item Maximum memory used while solving a Sudoku.
\end{enumerate}
 
For each criteria, we record its average value and its maximum value over each dataset for each algorithm.

All algorithms are implemented in Python and all of the relevant code is available in the GitHub repository associated with this paper.\footnote{https://github.com/doegan32/CS7IS2-AI-Group-Poject} All algorithms were run on a whatever computer with whatever specs.

\subsection{Results}
Tables 1, 2, and 3 display the results of our analysis. In each table, \emph{mean} is the mean value recorded across all puzzles in the dataset while \emph{max} is the maximum value recorded across all puzzles the dataset. 
\begin{table}[ht]
\centering
\begin{tabular}{ |c c c c c c c c| } 
 \hline
 \multicolumn{7}{|c|}{\textbf{Number of Nodes in Search Tree}} \\
  \hline
   & \multicolumn{2}{|c|}{Easy} &  \multicolumn{2}{c|}{Medium} &  \multicolumn{2}{c|}{Difficult} \\
 \multicolumn{1}{|c|}{}  & mean &  \multicolumn{1}{c|}{max}& mean &  \multicolumn{1}{c|}{max} & mean &  \multicolumn{1}{c|}{max} \\
  \hline
\multicolumn{1}{|l|}{Backtracking / Depth first search (baseline)} & $134$ & \multicolumn{1}{c|}{$3{,}994$} & $111$ & \multicolumn{1}{c|}{$2{,}027$} & $503{,}831$ & \multicolumn{1}{c|}{$25{,}158{,}597$}\\ 
\multicolumn{1}{|l|}{Breadth first search (BFS)} & $39{,}246$ & \multicolumn{1}{c|}{$270{,}649$} & $81{,}816$ & \multicolumn{1}{c|}{$511{,}645$} & - & \multicolumn{1}{c|}{-} \\ 
\multicolumn{1}{|l|}{Backtracking with AC-3} & $14{,}685$ & \multicolumn{1}{c|}{$145{,}169$} & $38{,}318$ & \multicolumn{1}{c|}{$346{,}922$} & $6{,}510{,}868$ & \multicolumn{1}{c|}{$116{,}038{,}117$} \\ 
\multicolumn{1}{|l|}{Backtracking with MRV}& $\textbf{53}$ & \multicolumn{1}{c|}{$\textbf{59}$} & $\textbf{57}$ & \multicolumn{1}{c|}{$\textbf{59}$} & $\textbf{62}$ & \multicolumn{1}{c|}{$\textbf{64}$} \\ 
\multicolumn{1}{|l|}{Algorithm X} & 85 & \multicolumn{1}{c|}{113} & 110 & \multicolumn{1}{c|}{228} & 416 & \multicolumn{1}{c|}{2268} \\ 
 \hline
\end{tabular}
\caption{Table illustrating number of nodes expanded in search trees for each algorithm}
\label{table:1}
\end{table}

\begin{table}[ht]
\centering
\begin{tabular}{ |c c c c c c c c| } 
 \hline
 \multicolumn{7}{|c|}{\textbf{Memory Usage (in MBs)}} \\
  \hline
   & \multicolumn{2}{|c|}{Easy} &  \multicolumn{2}{c|}{Medium} &  \multicolumn{2}{c|}{Difficult} \\
 \multicolumn{1}{|c|}{}  & mean &  \multicolumn{1}{c|}{max}& mean &  \multicolumn{1}{c|}{max} & mean &  \multicolumn{1}{c|}{max} \\
  \hline
\multicolumn{1}{|l|}{Backtracking / Depth first search (baseline)}& $0.0146$ & \multicolumn{1}{c|}{$0.156$} & $0.0095$ & \multicolumn{1}{c|}{\textbf{0.0181}} & $0.014$ & \multicolumn{1}{c|}{$0.1593$} \\ 
\multicolumn{1}{|l|}{Breadth first search (BFS)} &$96.8672$  & \multicolumn{1}{c|}{$107.0738$} & $126.9245$ & \multicolumn{1}{c|}{164.526} & - & \multicolumn{1}{c|}{-} \\ 
\multicolumn{1}{|l|}{Backtracking with AC-3} & $0.1674$ & \multicolumn{1}{c|}{$0.2829$} & $0.1659$ & \multicolumn{1}{c|}{$0.2338$} & $0.1659$ & \multicolumn{1}{c|}{$0.2336$} \\ 
\multicolumn{1}{|l|}{Backtracking with MRV}& $0.0093$ & \multicolumn{1}{c|}{$0.1597$} & $0.0109$ & \multicolumn{1}{c|}{$0.1544$} & $0.0142$ & \multicolumn{1}{c|}{$0.1548$} \\ 
\multicolumn{1}{|l|}{Algorithm X} & \textbf{0.0054} & \multicolumn{1}{c|}{\textbf{0.0443}} & \textbf{0.0054} & \multicolumn{1}{c|}{$0.0448$} & \textbf{0.006} & \multicolumn{1}{c|}{\textbf{0.0448}} \\ 
 \hline
\end{tabular}
\caption{Table illustrating memory usage for each algorithm}
\label{table:2}
\end{table}

\begin{table}[ht]
\centering
\begin{tabular}{ |c c c c c c c c| } 
 \hline
 \multicolumn{7}{|c|}{\textbf{Time Taken (in seconds)}} \\
  \hline
   &\multicolumn{2}{|c|}{Easy} &  \multicolumn{2}{c|}{Medium} &  \multicolumn{2}{c|}{Difficult} \\
 \multicolumn{1}{|c|}{}  & mean &  \multicolumn{1}{c|}{max}& mean &  \multicolumn{1}{c|}{max} & mean &  \multicolumn{1}{c|}{max} \\
  \hline
\multicolumn{1}{|l|}{Backtracking / Depth first search (baseline)}& \textbf{0.0047} & \multicolumn{1}{c|}{$0.1250$} & \textbf{0.0034} & \multicolumn{1}{c|}{$0.0625$} & $15.5403$ & \multicolumn{1}{c|}{$776.0456$} \\ 
\multicolumn{1}{|l|}{Breadth first search (BFS)} & $6.2722$ & \multicolumn{1}{c|}{$43.1273$} & $12.8609$ & \multicolumn{1}{c|}{$79.6345$} & - & \multicolumn{1}{c|}{-} \\ 
\multicolumn{1}{|l|}{Backtracking with AC-3} & $0.2718$ & \multicolumn{1}{c|}{$2.7125$} & $0.643$ & \multicolumn{1}{c|}{$5.6237$} & $119.9591$ & \multicolumn{1}{c|}{$2{,}121.9081$} \\ 
\multicolumn{1}{|l|}{Backtracking with MRV}& $0.0353$ & \multicolumn{1}{c|}{$0.0469$} & $0.0387$ & \multicolumn{1}{c|}{$0.0469$} & $0.0437$ & \multicolumn{1}{c|}{$0.0625$} \\ 
\multicolumn{1}{|l|}{Algorithm X} & $0.0056$ & \multicolumn{1}{c|}{\textbf{0.0313}} & $0.0066$ & \multicolumn{1}{c|} {\textbf{0.0313}} & \textbf{0.0148} & \multicolumn{1}{c|}{\textbf{0.0613}} \\ 
 \hline
\end{tabular}
\caption{Table illustrating time taken for each algorithm}
\label{table:3}
\end{table}

\subsection{Discussion}
\subsubsection{Number of nodes:}
Backtracking with MRV was by far the best performing algorithm with respect to this criteria, significantly outperforming the baseline backtracking search across all three datasets. It required only 62 nodes on average for the difficult dataset. In contrast, the baseline required an average of 503,831 nodes for the difficult dataset. Algorithm X also significantly outperformed the baseline, requiring an average of 416 nodes on the difficult dataset. 
The AC-3 algorithm was able to completely solve some Sudoku puzzles in the easy dataset by itself, i.e. without any searching. Hence, AC-3 had a node count of zero for some puzzles. However, interestingly, when it was unable to fully solve a puzzle, AC-3 significantly diminished the performance of backtracking search. Adding AC-3 as a pre-processing step to backktracking search increased the average node count by almost 13 times for the difficult dataset. This was despite the variable domains being significantly reduced in size by AC-3 before searching began. Breadth first search performed very poorly, failing to solve any puzzles in the difficult dataset before running out of memory.

The difficulty of the Sudoku had a much smaller impact on the performance of backtracking with MRV than on the performance of all the other algorithms. Backtracking with MRV's average node count on the difficult dataset was only 1.17 times its average node count on the easy dataset. The corresponding figure for the baseline was 3,760. Similarly, backtracking with MRV's maximum node count on the difficult data set was 64, only 2 bigger than its average of 62. In contrast, the baseline's maximum node count on the difficult dataset was almost 50 times higher than its average.

\subsubsection{Memory usage:}
Algorithm X was the best performing algorithm with respect to this criteria. Its average memory usage was less than half that of the baseline on the easy and difficult dataset. The baseline and backtracking with MRV performed similarly on the difficult dataset. The baseline outperformed backtracking with MRV on the medium dataset while the opposite was the case on the easy dataset. Once again, the AC-3 algorithm diminished the performance of the backtracking search algorithm. Unsurprisingly, breadth first search was the worst performing algorithm by a significant margin.

The difficulty of the puzzles only had a significant impact on memory usage for breadth first search. The baseline and backtracking with AC-3 actually had a lower average maximum memory usage on the difficult dataset than on the easy dataset, while Algorithm X's average maximum memory usage on the difficult dataset was only 1.1 times the corrsponding figure for the easy dataset.

\subsubsection{Time taken:} In terms of average time per puzzle, no algorithm outperformed the baseline on the easy and medium datasets. However, the baseline's performance was significantly worse on the difficult dataset and it was easily outperformed by backtracking with MRV and Algorithm X. Once again, breadth first search and backtracking with AC-3 were the worst performing algorithms.

Backtracking with MRV's performance was the least effected by the difficulty of the puzzles. Its average time on the difficult dataset was 1.2 times its average time on the easy dataset. Its maxmimum time on the difficult dataset was 1.4 times its average time on the difficult dataset. The corresponding figures for Algorithm X are 2.6 and 4.1. For the baseline, the figures are 3,306 and 49.9.

\vspace{12pt}Taking all criteria into account and viewing performance on the difficult dataset as the most important, our analysis suggests that the best Sudoku solving algorithms are backtracking search with the minimum-remaining-values heuristic and Algorithm X. While backtracking with MRV easily wins in terms of node count, Algorithm X wins in terms of memory usage and time.

\section{Conclusions}
In this paper, we performed a survey of different algorithms that can be used to solve Sudoku. We analysed the performance of these algorithms on three datasets and according to three criteria - number of nodes in the search tree, time taken, and memory usage. Our results indicate that, of the algorithms tested, backtracking search with the minimum-remaining-values heuristic and Algorithm X are the best algorithms for solving Sudoku.

The only purely inference algorithm that we looked at was AC-3. This was only able to solve the simplest puzzles and surprisingly, it diminished the performance of backtracking search when used as a pre-processing step. It would be interesting to find out why this was the case. Another area for future work would be to look at the performance of other inference algorithms such as PC-3 which achieves path-consistency, a stronger condition than the arc-consistency achieved by AC-3.

\begin{thebibliography}{6}
%

\bibitem {norv:russ}
Norvig, P., Russell, S.: Artificial Intelligence A Modern Approach Third Edition. Pearson Education Limited (2016)

\bibitem {mack}
Mackworth, A.K.,Consistency in Networks of Relations.
In: Artificial Intelligence 1977 8(1):99-118. doi:10.1016/0004-3702(77)90007-8

\bibitem {bit:rein}
Bitner, J.R., Reingold, E.M.: Backtrack Programming Techniques.
In: Communications of the ACM, 18(11), 651-656. doi:10.1145/361219.361224

\bibitem {bit:junior}
Breadth-First Search, Depth-First Search and Backtracking Depth-First Search Algorithms by Fernando Rodrigues Junior College of Science and Engineering University of Minnesota - Twin Cities

\bibitem {norvig}
Solving Every Sudoku Puzzle. \url{https://norvig.com/sudoku.html}

\bibitem {moore}
Moore, Edward F. (1959). "The shortest path through a maze". Proceedings of the International Symposium on the Theory of Switching. Harvard University Press. pp. 285–292. As cited by Cormen, Leiserson, Rivest, and Stein

\bibitem{knuth2000dancing} 
Donald E. Knuth: Dancing links
\\\texttt{https://arxiv.org/abs/cs/0011047}

\bibitem{book_1975}
Richard M. Karp. Reducibility among combinatorial problems. Complexity of computer computations, Proceedings of a Symposium on the Complexity of Computer Computations, held March 20-22, 1972, at the IBM Thomas J. Watson Center, Yorktown Heights, New York, edited by Raymond E. Miller and James W. Thatcher, Plenum Press, New York and London 1972, pp. 85–103 


% @article{book_1975, title={Richard M. Karp. Reducibility among combinatorial problems. Complexity of computer computations, Proceedings of a Symposium on the Complexity of Computer Computations, held March 20-22, 1972, at the IBM Thomas J. Watson Center, Yorktown Heights, New York, edited by Raymond E. Miller and James W. Thatcher, Plenum Press, New York and London 1972, pp. 85–103.}, volume={40}, DOI={10.2307/2271828}, number={4}, journal={Journal of Symbolic Logic}, publisher={Cambridge University Press}, author={Book, Ronald V.}, year={1975}, pages={618–619}}



\end{thebibliography}
\end{document}